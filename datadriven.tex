%!TEX root = presentazionelancia.tex
\section{Part I: Data Driven Stream Join (kNN)}


\begin{frame}[t]
\frametitle{Part I: Data Driven Stream Join (kNN)}
    \begin{center}
    	\includegraphics<1>[width=1\textwidth]{figs/knn.png}
    \end{center}
\end{frame}



\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Parallel Workflow
		\item Theoretical Analysis
		\item Continuous kNN
		\item Experiment Result
		\item Conclusion
	\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item \textcolor{blue!20}{Parallel Workflow}
		\item \textcolor{blue!20}{Theoretical Analysis}
		\item \textcolor{blue!20}{Continuous kNN}
		\item \textcolor{blue!20}{Experiment Result}
		\item \textcolor{blue!20}{Conclusion}
	\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Introduction}
\begin{block}{Definition: kNN}
Given a set of query points $R$ and a set of reference points $S$, a \textbf{$k$ nearest neighbor join} is an operation which, for each point in $R$, discovers the $k$ nearest neighbors in $S$. 
\end{block}
\begin{itemize}
\item Query never changes
\item Data changes: GPS (2 Dimensions), Twitter (77 Dimensions), Images (128 Dimensions) etc.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Introduction: Basic Idea}
\begin{itemize}
\item Nested Loop -- Calculate the Distances (Complexity $O(n^2)$)
\end{itemize}
\vspace{-0.3in}
    \begin{center}
    	\includegraphics<1>[width=0.4\textwidth]{figs/nestedloop.png}
    \end{center}
\vspace{-0.3in}
\begin{itemize}
\item Sort -- Find the top k smallest distance for each element (Complexity: Quick Sort: $n \cdot log(n)$, Priority Queue: $log(n)$)
\end{itemize}
\vspace{-0.2in}
    \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/quicksort.jpg}
    \end{center}
\end{frame}


\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Parallel Workflow
		\item \textcolor{blue!20}{Theoretical Analysis}
		\item \textcolor{blue!20}{Continuous kNN}
		\item \textcolor{blue!20}{Experiment Result}
		\item \textcolor{blue!20}{Conclusion}
	\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Parallel Workflow}
\begin{itemize}
\item Data Preprocessing
\begin{itemize}
\item To reduce the dimension of data
\item To select central points of data clusters
\end{itemize}
\item \textcolor{blue!20}{Data Partitioning}
\begin{itemize}
\item \textcolor{blue!20}{Distance Based Partitioning Strategy}
\item \textcolor{blue!20}{Size Based Partitioning Strategy}
\end{itemize}
\item \textcolor{blue!20}{Computation}
\begin{itemize}
\item \textcolor{blue!20}{One Round Job}
\item \textcolor{blue!20}{Two Rounds jobs}
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Data Preprocessing --- Reducing the dimension of data}
Space Filling Curve (Z-Value) --- Project high dimensional data to 1 dimension while preserving the locality information
    \vspace{-0.2in}
    \begin{center}
    	\includegraphics<1>[width=0.5\textwidth]{figs/z-value.png}
    \end{center}
   \tiny{ [z-value]:  Efficient parallel kNN joins for large data in MapReduce, EDBT 2012,  Chi Zhang et. al.}
\end{frame}

\begin{frame}[t]
\frametitle{Data Preprocessing --- Reducing the dimension of data}
	Locality Sensitive Hashing (LSH) --- Map the neighbor points into the same buckets with a high probability using Hash Families
	\vspace{-0.3in}
	\begin{center}
    	\includegraphics<1>[width=0.5\textwidth]{figs/LSH.png}
    \end{center}
    \tiny{[LSH]:  RankReduce - processing K-Nearest Neighbor queries on top of MapReduce, LSDS-IR 2010, Aleksandar Stupar et. al.}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Data Preprocessing --- Reducing the dimension of data}
\textbf{To avoid the loss of information}

\begin{itemize}
\item \textbf{z-value: } Create several ``shifts" of data,  calculate z-value for each shift of datas

\item \textbf{LSH: } Use multiple hash families
\end{itemize}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Data Preprocessing --- Selecting central points (Pivots) of data clusters}
\textbf{Voronoi Diagram: }
\begin{columns}
\begin{column}{0.45\textwidth}
 	\begin{itemize}
\item Random Selection
\item Furthest Selection
\item K-Means Selection
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
 	\includegraphics<1>[width=1\textwidth]{figs/voronoi.png}
\end{column}
\end{columns}
\tiny{[voronoi]: Efficient Processing of k Nearest Neighbor Joins using MapReduce, VLDB 2012, Wei Lu et. al.}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Data Preprocessing --- To select central points (Pivots) of data clusters}
\begin{itemize}
\item \textbf{Random Selection: } generates a set of samples, calculates the pairwise distance of the points in the sample, and the sample with the biggest sum of distances is chosen as the set of pivots.

\item \textbf{Furthest Selection: } randomly chooses the first pivot, and calculates the furthest point to this chosen pivot as the second pivot, and so on until having the desired number of pivots.

\item \textbf{K-Means Selection: } applies the traditional k-means method on a data sample to update the centroid of each cluster as the new pivots in each step, until the set of pivots stabilizes. 
\end{itemize}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Parallel Workflow}
\begin{itemize}
\item Data Preprocessing
\begin{itemize}
\item To reduce the dimension of data
\item To select central points of data clusters
\end{itemize}
\item Data Partitioning
\begin{itemize}
\item Distance Based Partitioning Strategy
\item Size Based Partitioning Strategy
\end{itemize}
\item \textcolor{blue!20}{Computation}
\begin{itemize}
\item \textcolor{blue!20}{One Round Job}
\item \textcolor{blue!20}{Two Rounds jobs}
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Data Partitioning --- Basic Idea (Block Nested Loop)}
    \begin{center}
    	\includegraphics<1>[width=0.4\textwidth]{figs/randompartition.png}
    \end{center}
    \vspace{-0.3in}
    \textcolor{red}{Problem: } $n^2$ tasks for calculating pairwise distances; wastes a lot of hardware resources, and ultimately leads to low efficiency.
\end{frame}

\begin{frame}
\frametitle{Data Partitioning --- Motivation}
The key to improve the performance is to preserve spatial locality of objects when decomposing data for tasks.
    \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/join1.png}
    \end{center}
    \vspace{-0.3in}
        \begin{center}
    	\includegraphics<1>[width=0.8\textwidth]{figs/join2.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Data Partitioning --- Motivation}
    \begin{center}
    	\includegraphics<1>[width=0.8\textwidth]{figs/advancedpartition.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Data Partitioning --- Distance Based Partitioning Strategy --- Voronoi Diagram}
This strategy wants to have the most relevant points in each partition. 
   
\begin{columns}
\begin{column}{0.45\textwidth}
 	\begin{itemize}
\item[1] Selection Pivots in R
\item[2] Partition R
\item[3] Upper Bound in R
\item[4] Find corresponding partition in S for each R
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
 	\includegraphics<1>[width=1\textwidth]{figs/V1.png}
 	\includegraphics<2>[width=1\textwidth]{figs/V2.png}
 	\includegraphics<3>[width=1\textwidth]{figs/V3.png}
 	\includegraphics<4>[width=1\textwidth]{figs/V4.png}
 	\includegraphics<5>[width=1\textwidth]{figs/V5.png}
\end{column}
\end{columns} 
   
\end{frame}

\begin{frame}
\frametitle{Data Partitioning --- Size Based Partitioning Strategy --- Z-Value}
This strategy wants to make each partition have equal size in order to achieve a good load balance.

\begin{columns}
\begin{column}{0.45\textwidth}
 	\begin{itemize}
\item[1] A Sample of R
\item[2] Find Quantiles in the Sample as the Bounds of the Partitions in R
\item[3] Find corresponding partition in S for each R
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
 	\includegraphics<1>[width=1\textwidth]{figs/Z1.png}
 	\includegraphics<2>[width=1\textwidth]{figs/Z2.png}
 	\includegraphics<3>[width=1\textwidth]{figs/Z3.png}
\end{column}
\end{columns} 

\end{frame}

\begin{frame}
\frametitle{Data Partitioning --- Size Based Partitioning Strategy --- LSH}
    \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/lshpartition.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Computation}
\begin{itemize}
\item One job --- Direct give the global results
\item Two consecutive jobs --- First give the local results, then merge the local results into the global results
\end{itemize}

\textbf{Purpose: } using multiple rounds of jobs in order to reduce the number of elements to be sorted.
\end{frame}

\begin{frame}
\frametitle{Computation --- Example}
For each city in $R$, find the nearest city in $S$.
	\includegraphics<1>[width=0.6\textwidth]{figs/france.jpg}
\end{frame}

\begin{frame}
\frametitle{Computation --- Example}
\textbf{One Job:}
 \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/1job.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Computation --- Example}
\textbf{Two Jobs:}
 \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/2job.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Workflow}
 \begin{center}
    	\includegraphics<1>[width=1\textwidth]{figs/workflow.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Parallel Workflow
		\item Theoretical Analysis
		\begin{itemize}
\item Load Balance
\item Accuracy
\item Complexity
\end{itemize}
		\item \textcolor{blue!20}{Continuous kNN}
		\item \textcolor{blue!20}{Experiment Result}
		\item \textcolor{blue!20}{Conclusion}
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Load Balance}
\begin{block}{What is Load Balance?}
\begin{center}
$\left| R_i \right|$ $\times$ $\left| S_i \right|$ = $\left| R_j \right|$ $\times$ $\left| S_j \right|$
\end{center}
\end{block}

\begin{block}{Sub-Optimal Option}
\begin{center}
$\forall$ i $\neq$ j, 

if $\left| R_i \right|$ = $\left| R_j \right|$ or $\left| S_i \right|$ = $\left| S_j \right|$, 

then $\left| R_i \right|$ $\times$ $\left| S_i \right|$ $\approx$ $\left| R_j \right|$ $\times$ $\left| S_j \right|$
\end{center}
\end{block}

\end{frame}


\begin{frame}
\frametitle{Load Balance}
\vspace{-0.1in}
\begin{block}{if $\left| R_i \right|$ = $\left| R_j \right|$, the Worst Case Complexity is:}
$\mathcal{O}\left(\left|R_i\right| \times log\left|S_i\right|\right) = \mathcal{O}\left(\frac{\left|R\right|}{p} \times log\left|S\right|\right)$
\end{block}
\vspace{-0.2in}
\begin{block}{if $\left| S_i \right|$ = $\left| S_j \right|$, the Worst Case Complexity is:}
$\mathcal{O}\left(\left|R_i\right| \times log\left|S_i\right|\right) = \mathcal{O}\left(\left|R\right|\times log\frac{\left|S\right|}{p}\right)$
\end{block}
\vspace{-0.2in}
$p$~$\ll \left|S\right|$ $\Rightarrow$ $\left| R_i \right|$ = $\left| R_j \right|$ is better.

\textcolor{red}{All advanced partitioning strategies first partition R into equal sized partitions, then find the corresponding S for each R.}
\end{frame}

\begin{frame}
\frametitle{Accuracy}
The lack of accuracy is the direct consequence of techniques to reduce the dimensionality with techniques of as z-values and LSH.
\begin{itemize}
\item \textbf{Z-Value}
\begin{itemize}
\item Depends on k
\item Shift of data --- move data in the direction of a random vector
\item Increase the number of shifts of data will decrease the error rate
\end{itemize}
\item \textbf{LSH}
\begin{itemize}
\item Depends on parameter tuning
\item Increase the number of hash functions will decrease the error rate
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Complexity}

\begin{itemize}
\item \textbf{The number of MapReduce jobs: } starting a job requires some initial steps.

\item \textbf{The number of Map tasks and Reduce tasks used to calculate $k$NN$\left(R_i \ltimes S\right)$: } the larger this number is, the more information is exchanged through the network.

\item \textbf{The number of final candidates for each object $r_i$: }  Finding the top k results is very time consuming. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Main Overhead }
\begin{itemize}
\item Communication overhead:
\begin{itemize}
\item the amount of data transmitted over the network
\end{itemize}
\item Computation overhead:
\begin{itemize}
\item computing the distances
\item finding the k smallest distances
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Parallel Workflow
		\item Theoretical Analysis
		\item Continuous kNN
		\item \textcolor{blue!20}{Experiment Result}
		\item \textcolor{blue!20}{Conclusion}
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sliding Window Model --- Motivation}
\begin{itemize}
\item Unbounded streams can not be wholly stored in bounded memory
\item New items in a stream are more relevant than older ones.
\end{itemize}
\vspace{-0.2in}
\begin{block}{Sliding Window Model}
Maintaining a moving window of the most recent elements in the stream
\end{block}
\vspace{-0.2in}
\begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/stream.png}
 \end{center}
\end{frame}

\begin{frame}
\frametitle{Sliding Window --- Two Strategies}
\begin{itemize}
\item \textbf{Re-Execution Strategy}
\begin{itemize}
\item[-] \textbf{Eager Re-execution Strategies} --- Generating new results right after each new data arrives
\item[-] \textbf{Lazy Re-execution Strategies} --- Re-Executing the query periodically
\end{itemize}
\item \textbf{Data Invalidation Strategy}
\begin{itemize}
\item[-] \textbf{Eager Invalidation Strategies} --- Scanning and moving forward the sliding window upon arrival of new data
\item[-] \textbf{Lazy Re-execution Strategies} --- Removing old data periodically and require more memory to store data waiting for expiration
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sliding Window --- Two Strategies}
\begin{itemize}
\item \textbf{Re-Execution Strategy}
\begin{itemize}
\item[-] Eager Re-execution Strategies
\item[-] \textcolor{red}{Lazy Re-execution Strategies}
\end{itemize}
\item \textbf{Data Invalidation Strategy}
\begin{itemize}
\item[-] Eager Expiration Strategies
\item[-] \textcolor{red}{Lazy Invalidation Strategies}
\end{itemize}
\end{itemize}
\vspace{-0.2in}
\textcolor{red}{Re-Execution and Expiration Period --- Generation}
\vspace{-0.2in}
\begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/12.png}
 \end{center}
\end{frame}

\begin{frame}
\frametitle{Different types of dynamic kNN joins}
\begin{itemize}
\item Static R and Dynamic S (SRDS)
\begin{itemize}
\item[-] Exists rarely in real applications. 
\item[-] Reuse the parallel methods
\end{itemize}
\item Dynamic R and Static S (DRSS)
\begin{itemize}
\item[-] Most used scenario in real applications
\item[-] Reuse Random Partition method
\end{itemize}
\item Dynamic R and Dynamic S (DRDS)
\begin{itemize}
\item[-] General situation
\item[-] Basic Method + Advanced Method
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Dynamic R and Dynamic S --- Basic Method (Sliding Block Nested Loop)}
\textbf{$n^2$ tasks for each generation}
\begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/slidingradompartition.png}
 \end{center}
\end{frame}

\begin{frame}
\frametitle{Dynamic R and Dynamic S --- Advanced Method (Naive Bayes Partitioning)}
\textbf{n tasks for each generation: }partition the new data items without moving the old ones.
\vspace{-0.2in}
\begin{block}{Naive Bayes Theory}
Given two independent events A and x, the conditional probability of given x and A occurs is:
\begin{equation}
P(A|x) = \frac{P(x|A) \cdot P(A)}{P(x)}
\end{equation}
\end{block}
\vspace{-0.2in}
The new data x will be partitioned to the partition with the highest conditional probability.
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{DRDS --- Advanced Method --- Naive Bayes Partitioning}
We consider the n partitions from $N_1$ to $N_n$ as n different classes and the already partitioned data as training set. 
The probability that a new point $x$ belongs to $N_i$ is:
\begin{equation}
P(N_i | x) = \frac{P(x | N_i) \cdot P(N_i)}{P(x)}
\end{equation}

And $x$ should be assigned to the partition $N_y$ which has the biggest probability:
\begin{equation}
x \in N_y, \  where \ P(N_y|x) = max\{P(N_1|x), P(N_2|x), ..., P(N_n|x)\}
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Naive Bayes Partitioning}
Partitioning Problem = The calculation of $P(N_i|x)$
\begin{equation}
P(N_i|x) = P(N_i|(l_1(x), l_2(x), ..., l_p(x)))
\end{equation}
According to Bayes' Theorem:
\begin{equation}
= \frac{P( (l_1(x), l_2(x), ..., l_p(x))|N_i )}{P(  l_1(x), l_2(x), ..., l_p(x) )}
\end{equation}
Since $l_1$, $l_2$, ..., $l_p$ are independent, we have:
\begin{equation}
= \frac{ P(l_1(x)|N_i) \cdot P( l_2(x)|N_i ) ...  \cdot P(l_p(x)|N_i)   \cdot P(N_i)}{P(l_1(x)) \cdot P(l_2(x)) ... \cdot P(l_p(x))   }
\end{equation}

$P(l_j(x)|N_i)$ is the probability of the appearance of $l_j(x)$ on $N_i$, and this probability is decided by the distribution of data on $N_i$.
\end{frame}
\end{comment}


\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Parallel Workflow
		\item Theoretical Analysis
		\item Continuous kNN
		\item Experiment Result
		\item \textcolor{blue!20}{Conclusion}
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experiment Setting}
\textbf{Cluster Setting}
\vspace{-0.1in}
\begin{itemize}
\item The experiments were run on two clusters of Grid'5000 with Hadoop 1.3

\item The number of replications for each split of data is set to 3

\item The number of slots of each node is 1
\end{itemize}
\textbf{Datasets}
\vspace{-0.1in}
\begin{itemize}
\item \textbf{OpenStreetMap} Geo dataset contains geographic XML data in two dimensions  --- Low Dimension
\item \textbf{Catech 101} It is a public set of images, which contains 101 categories of pictures of different objects. (Speeded Up Robust Features --- 128 dimensions) --- High Dimension
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Methods Evaluated}
\begin{itemize}
\item \textbf{H-BkNNJ} Naive Method -- Without preprocessing and partitioning -- One Job
\item \textbf{H-BNLJ} Block Nested Loop -- Without preprocessing and partitioning -- Two Jobs
\item \textbf{PGBJ} Based on Voronoi -- Preprocessing: Select Pivots -- Distance Based Partitioning -- One Job
\item \textbf{H-zkNNJ} Based on Z-Value -- Preprocessing: z-value -- Size Based Partitioning -- Two Jobs
\item \textbf{RankReduce} Based on LSH -- Preprocessing: LSH -- Size Based Partitioning -- Two Jobs
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Evaluations}
\textbf{Impacts:  (x axis)}
\begin{itemize}
\item Impact of input data size
\item Impact of k
\item Impact of Dimension and Dataset
\end{itemize}
\textbf{Measures: (y axis)}
\begin{itemize}
\item Execution Time
\item Communication Overhead
\item Recall and Precision
\end{itemize}
\textbf{Practical Analysis}
\end{frame}

\begin{frame}
\frametitle{Evaluation Result --- Verify the theoretical Analysis}
Execution Time for Geo dataset (2 dimensions): 

\begin{columns}
\begin{column}{0.45\textwidth}
 	\begin{itemize}
\item H-BkNNJ: Naive
\item H-BNLJ: Block Nested Loop
\item PGBJ: Voronoi
\item H-zkNNJ: z-value
\item RankReduce: LSH
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics<1>[width=1.2\textwidth]{figs/time.pdf}
\end{column}
\end{columns} 

\end{frame}

\begin{frame}
\frametitle{Evaluation Result --- Surprise}
Execution Time for Image dataset (128 dimensions):
\begin{columns}
\begin{column}{0.45\textwidth}
 	\begin{itemize}
\item H-BkNNJ: Naive
\item H-BNLJ: Block Nested Loop
\item PGBJ: Voronoi
\item H-zkNNJ: z-value
\item RankReduce: LSH
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
	\includegraphics<1>[width=1.2\textwidth]{figs/time_surf.pdf}
\end{column}
\end{columns} 
\end{frame}

\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Parallel Workflow
		\item Theoretical Analysis
		\item Continuous kNN
		\item Experiment Result
		\item Conclusion
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{itemize}

\item Parallel Workflow for kNN Join

\item Continuous kNN Join for Data Streams

\item Theoretical Analysis

\item Experimental Analysis for 5 Methods

\end{itemize}

\end{frame}


\begin{comment}
\begin{frame}
\frametitle{Conclusion}
\vspace{-0.2in}
\begin{table}
 	\begin{center}\renewcommand{\arraystretch}{1.2}
 	\resizebox{\textwidth}{!}{
 		\begin{tabular}{|c|c|c|c|}
 			\hline
 			\textbf{Algorithm}  & \textbf{Advantage}  &  \textbf{Shortcoming}   & \textbf{Typical Usecase} \\
 			 \hline
 			\textbf{H-BkNNJ} 	& 
 				\begin{tabular}[c]{@{}c@{}}
 					Trivial to implement  
 				\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 					1. Breaks very quickly \\ 
 					2. Optimal parallelism difficult\\ 
 					to achieve a priori
 				\end{tabular} &
 				\begin{tabular}[c]{@{}c@{}}
 			    	Any tiny and low dimension dataset\\
 			    	($\sim$ 25000 records)
 			    \end{tabular} \\ \hline
 			\textbf{H-BNLJ}     &
 				\begin{tabular}[c]{@{}c@{}}
 				 	Easy to implement
 				\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 				 	1. Slow\\
 				 	2. Very large communication overhead 
 				\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 				 	Any small/medium dataset\\
 				 	($\sim$ 100000 records)
 				\end{tabular} \\ \hline
 			\textbf{PGBJ}       & 
	 			\begin{tabular}[c]{@{}c@{}}
 				   	1. Exact solution\\ 
 				   	2. Lowest disk usage\\ 
 			       	3. No impact on communication\\
 			       	overhead with the increase of $k$
	 			\end{tabular}        & 
	 			\begin{tabular}[c]{@{}c@{}}
	 				1. Cannot finish in reasonable time\\ 
	 				for large datasets\\ 
	 				2. Poor performance for high\\
	 				dimension data\\
	 				3. Large communication overhead\\
	 				4. Performance highly depends on\\ 
	 				the quality of a priori chosen pivots
	 			\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 					1. Medium/large dataset for\\
 					low/medium dimension\\
 					2. Exact results
 				\end{tabular} \\ \hline
 			\textbf{H-zkNNJ}    &
	 			\begin{tabular}[c]{@{}c@{}}
	 			    1. Fast\\ 
	 			    2. Does not require a priori parameter\\ 
	 			    tuning \\ 
	 			    3. More precise for large k \\ 
	 			    4. Always give the right number of $k$	 	
	 	        \end{tabular} & 
	 	        \begin{tabular}[c]{@{}c@{}}
	 	        	1. High disk usage\\ 
	 	        	2. Slow for large dimension \\ 
	 	        	3. Very high space requirement ratio \\ 
	 	        	for small values of $k$
 				\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 					1. Large dataset of small dimension\\
 					2. High values of $k$\\
 			        3. Approximate results
 				\end{tabular} \\ \hline
 			\textbf{RankReduce} & 
 			\begin{tabular}[c]{@{}c@{}}
 				1. Fast\\ 
 				2. Low footprint on disk usage
 			\end{tabular} & 
 			\begin{tabular}[c]{@{}c@{}}
 				1. Fine parameter tuning required with\\ 
 				experimental set up \\
 				2. Multiple hash functions needed for\\
 				acceptable recall \\
 				3. Different quality metrics to consider\\
 				(recall + precision)
 		   \end{tabular}                                           
 			& \begin{tabular}[c]{@{}c@{}}
 				1. Large dataset of any dimension \\
 				2. Approximate results \\
 				3. Room for parameter tuning
 			\end{tabular} \\ \hline
 		\end{tabular}} 
 	\end{center}
 \end{table}
\end{frame}
\end{comment}