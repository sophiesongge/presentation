%!TEX root = presentazionelancia.tex
\section{Part I: Data Driven Stream Join (kNN)}

\begin{comment}
\begin{frame}[t]
\frametitle{Part I: Data Driven Stream Join (kNN)}
    \begin{center}
    	\includegraphics<1>[width=1\textwidth]{figs/knn.png}
    \end{center}
\end{frame}
\end{comment}



\begin{frame}
\frametitle{Part I: Data Driven Stream Join (kNN)}
	\begin{itemize}
		\item Introduction
		\item Survey about Parallel Solutions on MapReduce
		\begin{itemize}
		\item Parallel Workflow
		\item Theoretical Analysis
		\item Experiment Result
		\end{itemize}
		\item Continuous kNN
		\item Conclusion
	\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item \textcolor{blue!20}{Survey about Parallel Solutions}
		\begin{itemize}
		\item \textcolor{blue!20}{Parallel Workflow}
		\item \textcolor{blue!20}{Theoretical Analysis}
		\item \textcolor{blue!20}{Experiment Result}
		\end{itemize}
		\item \textcolor{blue!20}{Continuous kNN}
		\item \textcolor{blue!20}{Conclusion}
	\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Introduction}
\begin{block}{Definition: kNN}
Given a set of query points $R$ and a set of reference points $S$, a \textbf{$k$ nearest neighbor join} is an operation which, for each point in $R$, discovers the $k$ nearest neighbors in $S$. 
\end{block}
\end{frame}

\begin{frame}
\frametitle{An Example of kNN Join}
For each city in $R$, find the nearest city in $S$. (1-NN)


\begin{columns}
\begin{column}{0.5\textwidth}
 	\includegraphics<1>[width=1.2\textwidth]{figs/france.jpg}
\end{column}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item \textbf{R: }Query Set
\item \textbf{S: }Reference Set
\end{itemize}
\end{column}
\end{columns}
\end{frame}


\begin{frame}
\begin{itemize}
\item Query never changes
\item Data format changes: GPS (2 Dimensions), Twitter (77 Dimensions), Images (128 Dimensions) etc.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Introduction: Basic Idea}
\begin{itemize}
\item Nested Loop -- Calculate the Distances 
\end{itemize}
\vspace{-0.3in}
    \begin{center}
    	\includegraphics<1>[width=0.4\textwidth]{figs/nestedloop.png}
    \end{center}
\vspace{-0.3in}
\begin{itemize}
\item Sort -- Find the top k smallest distance for each element 
\end{itemize}

\textcolor{red}{Problem: Data Intensive and Computation Intensive}
\end{frame}


\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Survey about Parallel Solutions
		\begin{itemize}
		\item Parallel Workflow
		\item Theoretical Analysis
		\item Experiment Result
		\end{itemize}
		\item \textcolor{blue!20}{Continuous kNN}
		\item \textcolor{blue!20}{Conclusion}
	\end{itemize}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Parallel Workflow}
\begin{itemize}
\item Data Preprocessing
\begin{itemize}
\item To reduce the dimension of data
\item To select central points of data clusters
\end{itemize}
\item \textcolor{blue!20}{Data Partitioning}
\begin{itemize}
\item \textcolor{blue!20}{Distance Based Partitioning Strategy}
\item \textcolor{blue!20}{Size Based Partitioning Strategy}
\end{itemize}
\item \textcolor{blue!20}{Computation}
\begin{itemize}
\item \textcolor{blue!20}{One Round Job}
\item \textcolor{blue!20}{Two Rounds jobs}
\end{itemize}
\end{itemize}
\end{frame}
\end{comment}


\begin{frame}
\frametitle{First Result: Parallel Workflow}

\begin{center}
	\includegraphics<1>[width=1\textwidth]{figs/pw0.png}
\end{center}
\end{frame}



\begin{frame}
\frametitle{Data Preprocessing : Reduce the dimension of data}
\textbf{The curse of dimensionality: } Too difficult to calculate in high dimension space.

\textbf{Solution: } Project data from high dimension space to a low dimension one while preserving the locality information
\end{frame}

\begin{frame}
\frametitle{Data Preprocessing --- Reducing the dimension of data}
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Space Filling Curve (Z-Value)}
 	 	\includegraphics<1>[width=0.6\textwidth]{figs/z-value.png}
\end{column}

\begin{column}{0.6\textwidth}
\textbf{LSH: Locality Sensitive Hashing}
\includegraphics<1>[width=0.7\textwidth]{figs/LSH.png}
\end{column}
\end{columns}
\vspace{0.1in}
 \tiny{ [z-value]:  Efficient parallel kNN joins for large data in MapReduce, EDBT 2012,  Chi Zhang et. al.}
 
  \tiny{[LSH]:  RankReduce - processing K-Nearest Neighbor queries on top of MapReduce, LSDS-IR 2010, Aleksandar Stupar et. al.}
\end{frame}

\begin{comment}
\begin{frame}[t]
\frametitle{Data Preprocessing --- Reducing the dimension of data}
	Locality Sensitive Hashing (LSH) --- Map the neighbor points into the same buckets with a high probability using Hash Families
	\vspace{-0.3in}
	\begin{center}
    	\includegraphics<1>[width=0.5\textwidth]{figs/LSH.png}
    \end{center}
    \tiny{[LSH]:  RankReduce - processing K-Nearest Neighbor queries on top of MapReduce, LSDS-IR 2010, Aleksandar Stupar et. al.}
\end{frame}
\begin{comment}

\begin{comment}
\begin{frame}
\frametitle{Data Preprocessing --- Reducing the dimension of data}
\textbf{To avoid the loss of information}

\begin{itemize}
\item \textbf{z-value: } Create several ``shifts" of data,  calculate z-value for each shift of datas

\item \textbf{LSH: } Use multiple hash families
\end{itemize}
\end{frame}
\end{comment}




\begin{comment}
\begin{frame}
\frametitle{Data Preprocessing --- Selecting central points (Pivots) of data clusters}
\textbf{Voronoi Diagrams: }
\begin{columns}
\begin{column}{0.45\textwidth}
 	\begin{itemize}
\item Random Selection
\item Furthest Selection
\item K-Means Selection
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
 	\includegraphics<1>[width=1\textwidth]{figs/voronoi.png}
\end{column}
\end{columns}
\tiny{[voronoi]: Efficient Processing of k Nearest Neighbor Joins using MapReduce, VLDB 2012, Wei Lu et. al.}
\end{frame}
\begin{comment}


\begin{comment}
\begin{frame}
\frametitle{Data Preprocessing --- To select central points (Pivots) of data clusters}
\begin{itemize}
\item \textbf{Random Selection: } generates a set of samples, calculates the pairwise distance of the points in the sample, and the sample with the biggest sum of distances is chosen as the set of pivots.

\item \textbf{Furthest Selection: } randomly chooses the first pivot, and calculates the furthest point to this chosen pivot as the second pivot, and so on until having the desired number of pivots.

\item \textbf{K-Means Selection: } applies the traditional k-means method on a data sample to update the centroid of each cluster as the new pivots in each step, until the set of pivots stabilizes. 
\end{itemize}
\end{frame}
\end{comment}

\begin{comment}
\begin{frame}
\frametitle{Parallel Workflow}
\begin{itemize}
\item Data Preprocessing
\begin{itemize}
\item To reduce the dimension of data
\item To select central points of data clusters
\end{itemize}
\item Data Partitioning
\begin{itemize}
\item Distance Based Partitioning Strategy
\item Size Based Partitioning Strategy
\end{itemize}
\item \textcolor{blue!20}{Computation}
\begin{itemize}
\item \textcolor{blue!20}{One Round Job}
\item \textcolor{blue!20}{Two Rounds jobs}
\end{itemize}
\end{itemize}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Parallel Workflow}
\begin{center}
	\includegraphics<1>[width=1\textwidth]{figs/pw1.png}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Data Partitioning}

\begin{columns}
\begin{column}{0.5\textwidth}

 	 		\includegraphics<1>[width=0.8\textwidth]{figs/datapartition.png}

\end{column}

\begin{column}{0.5\textwidth}
\textbf{Purpose: } Cut big data sets into smaller ones
\end{column}
\end{columns}

\end{frame}


\begin{frame}
\frametitle{Data Partitioning --- Basic Idea (Block Nested Loop)}
\vspace{-0.1in}
    \begin{center}
    	\includegraphics<1>[width=0.5\textwidth]{figs/randompartition.png}
    \end{center}
    \vspace{-0.3in}
    \textcolor{red}{Problem: } $n^2$ tasks for calculating pairwise distances; wastes a lot of hardware resources, and ultimately leads to low efficiency.
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Data Partitioning --- Motivation}
The key to improve the performance is to preserve locality of objects when decomposing data for tasks.
    \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/join1.png}
    \end{center}
    \vspace{-0.3in}
        \begin{center}
    	\includegraphics<1>[width=0.8\textwidth]{figs/join2.png}
    \end{center}
\end{frame}
\end{comment}


\begin{frame}
\frametitle{Data Partitioning --- Motivation}
\textbf{Purpose: } Reduce the task number from $n^2$ to n
\vspace{-0.2in}
    \begin{center}
    	\includegraphics<1>[width=0.8\textwidth]{figs/advancedpartition.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Data Partitioning --- Distance Based Partitioning Strategy --- Voronoi Diagrams}
This strategy wants to have the most relevant points in each partition. 
   
\begin{columns}
\begin{column}{0.45\textwidth}
 	\begin{itemize}
\item[1] Partition Query Set R
\item[2] Use same diagrams to partition S
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
 	\includegraphics<1>[width=1\textwidth]{figs/vo1.png}
 	\includegraphics<2>[width=1\textwidth]{figs/vo2.png}
 	%\includegraphics<3>[width=1\textwidth]{figs/V3.png}
 	%\includegraphics<4>[width=1\textwidth]{figs/V4.png}
 	%\includegraphics<5>[width=1\textwidth]{figs/V5.png}
\end{column}
\end{columns} 
Take neighbor cells
\end{frame}

\begin{frame}
\frametitle{Data Partitioning --- Size Based Partitioning Strategy --- Z-Value (or LSH)}
This strategy wants to make each partition have equal size in order to achieve a good load balance.

\begin{columns}
\begin{column}{0.45\textwidth}
 	\begin{itemize}
\item[1] A Sample of R
\item[2] Partition the sample into equal sized partitions
\item[3] Find corresponding partition in S for each R
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
 	\includegraphics<1>[width=1\textwidth]{figs/Z1.png}
 	\includegraphics<2>[width=1\textwidth]{figs/Z2.png}
 	\includegraphics<3>[width=1\textwidth]{figs/Z3.png}
\end{column}
\end{columns} 
Take 3 partitions
\end{frame}
\begin{frame}
\frametitle{Parallel Workflow}
\begin{center}
	\includegraphics<1>[width=1\textwidth]{figs/pw2.png}
\end{center}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Data Partitioning --- Size Based Partitioning Strategy --- LSH}
    \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/lshpartition.png}
    \end{center}
    Take neighbor buckets
\end{frame}
\end{comment}



\begin{frame}
\frametitle{Computation}
\begin{itemize}
\item One job --- Directly give the global results
\item Two consecutive jobs --- First give the local results, then merge the local results into the global results
\end{itemize}

\textbf{Purpose: } reduce the number of elements to be sorted.
\end{frame}

\begin{frame}
\frametitle{Computation --- Example}
For each city in $R$, find the nearest city in $S$. (1-NN)
	\includegraphics<1>[width=0.6\textwidth]{figs/france.jpg}
\end{frame}

\begin{frame}
\frametitle{Computation --- Example}
\textbf{One Job:}
 \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/1job.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Computation --- Example}
\textbf{Two Jobs:}
 \begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/2job.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Parallel Workflow}
 \begin{center}
    	\includegraphics<1>[width=1\textwidth]{figs/workflow.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Result 2: Theoretical Analysis}

		\begin{itemize}
\item Load Balance
\item Accuracy
\item Complexity
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Load Balance}
 	\begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/lb.png}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Sub-Optimal Load Balance}
Partition one, deduce the other:
\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{(1) Partition R into equal-sized}
 	\includegraphics<1>[width=1\textwidth]{figs/rpartition.png}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{(2) Partition S into equal-sized}
 	\includegraphics<1>[width=1\textwidth]{figs/spartition.png}
\end{column}
\end{columns}
\textcolor{red}{(1) is better in the worst case.}
\end{frame}


\begin{comment}
\begin{frame}
\frametitle{Load Balance}
\begin{block}{What is Load Balance?}
\begin{center}
$\left| R_i \right|$ $\times$ $\left| S_i \right|$ = $\left| R_j \right|$ $\times$ $\left| S_j \right|$
\end{center}
\end{block}

\begin{block}{Sub-Optimal Option}
\begin{center}
$\forall$ i $\neq$ j, 

if $\left| R_i \right|$ = $\left| R_j \right|$ or $\left| S_i \right|$ = $\left| S_j \right|$, 

then $\left| R_i \right|$ $\times$ $\left| S_i \right|$ $\approx$ $\left| R_j \right|$ $\times$ $\left| S_j \right|$
\end{center}
\end{block}

\end{frame}
\end{comment}


\begin{comment}
\begin{frame}
\frametitle{Load Balance}
\vspace{-0.1in}
\begin{block}{if $\left| R_i \right|$ = $\left| R_j \right|$, the Worst Case Complexity is:}
\begin{center}
$ \mathcal{O}\left(\frac{\left|R\right|}{p} \times log\left|S\right|\right)$
\end{center}
\end{block}
\vspace{-0.2in}
\begin{block}{if $\left| S_i \right|$ = $\left| S_j \right|$, the Worst Case Complexity is:}
\begin{center}
$ \mathcal{O}\left(\left|R\right|\times log\frac{\left|S\right|}{p}\right)$
\end{center}
\end{block}
\vspace{-0.2in}
$p$~$\ll \left|S\right|$ $\Rightarrow$ $\left| R_i \right|$ = $\left| R_j \right|$ is better.

\textcolor{red}{All advanced partitioning strategies first partition R into equal sized partitions, then find the corresponding S for each R.}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Accuracy}
Accuracy: Number of correct results obtained
\begin{itemize}
\item \textbf{Z-Value}
\begin{itemize}
\item Depends on k
\item Shift of data --- move data in the direction of a random vector
\item Increase number of shifts of data to decrease error rate
\end{itemize}
\item \textbf{LSH}
\begin{itemize}
\item Depends on parameter tuning
\item Increase the number of hash functions to decrease error rate
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Complexity}

\begin{itemize}
\item \textbf{Number of MapReduce jobs: } starting a job requires some initial steps.

\item \textbf{Number of Map tasks and Reduce tasks used to calculate $k$NN$\left(R_i \ltimes S\right)$: } the larger this number, the more network communication

\item \textbf{Number of final candidates for each object $r_i$: }  Finding the top k results is very time consuming. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Result 3: Experimental Analysis}
\textbf{Cluster Setting}
\vspace{-0.1in}
\begin{itemize}
\item Two clusters of Grid'5000 with Hadoop 1.3 (3 replications, 1 slot per node)

\end{itemize}
\textbf{Datasets}
\vspace{-0.1in}
\begin{itemize}
\item \textbf{OpenStreetMap} Geo dataset contains geographic XML data in two dimensions  --- Low Dimension
\item \textbf{Caltech 101} It is a public set of images, which contains 101 categories of pictures of different objects. (Speeded Up Robust Features --- 128 dimensions) --- High Dimension
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Methods Evaluated}
\begin{itemize}
\item \textbf{H-BkNNJ} Naive Method -- Without preprocessing and partitioning -- One Job
\item \textbf{H-BNLJ} Block Nested Loop -- Without preprocessing and partitioning -- Two Jobs
\item \textbf{PGBJ} Based on Voronoi -- Preprocessing: Select Pivots -- Distance Based Partitioning -- One Job
\item \textbf{H-zkNNJ} Based on Z-Value -- Preprocessing: z-value -- Size Based Partitioning -- Two Jobs
\item \textbf{RankReduce} Based on LSH -- Preprocessing: LSH -- Size Based Partitioning -- Two Jobs
\end{itemize}
\end{frame}

\begin{comment}
\begin{frame}
\frametitle{Evaluations}
\textbf{Impacts:  (x axis)}
\begin{itemize}
\item Impact of input data size
\item Impact of k
\item Impact of Dimension and Dataset
\end{itemize}
\textbf{Measures: (y axis)}
\begin{itemize}
\item Execution Time
\item Communication Overhead
\item Recall and Precision
\end{itemize}
\textbf{Practical Analysis}
\end{frame}
\end{comment}

\begin{frame}
\frametitle{Evaluation Result --- Verify the theoretical Analysis}
Execution Time for Geo dataset (2 dimensions): 

\begin{columns}
\begin{column}{0.45\textwidth}
\includegraphics<1>[width=1\textwidth]{figs/method.png}
\end{column}
\begin{column}{0.55\textwidth}
\includegraphics<1>[width=1.2\textwidth]{figs/time.pdf}
\end{column}
\end{columns} 
\end{frame}

\begin{frame}
\frametitle{Evaluation Result --- Surprise}
Execution Time for Image dataset (128 dimensions):
\begin{columns}
\begin{column}{0.45\textwidth}
\includegraphics<1>[width=1\textwidth]{figs/method.png}
\end{column}
\begin{column}{0.55\textwidth}
	\includegraphics<1>[width=1.2\textwidth]{figs/time_surf.pdf}
\end{column}
\end{columns} 
\end{frame}


\begin{frame}
\frametitle{Conclusions of the survey}
\begin{itemize}
\item Clear and detailed view of the current algorithms for processing kNN on MapReduce

\item Fine grained analysis both theoretical and experimental for each algorithm to obtain the best performance.

\item Match algorithm with typical use case
\end{itemize}
\textbf{Limitation of the existing algorithms}
\begin{itemize}
\item Non of them can process data streams
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Survey about Parallel Solutions
		\begin{itemize}
		\item Parallel Workflow
		\item Theoretical Analysis
		\item Experiment Result
		\end{itemize}
		\item Continuous kNN
		\item \textcolor{blue!20}{Conclusion}
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sliding Window Model --- Motivation}
\begin{itemize}
\item Unbounded sequence of elements which can not be wholly stored in bounded memory
\item New items in a stream are more relevant than older ones.
\end{itemize}
\vspace{-0.2in}
\begin{block}{Sliding Window Model}
Maintaining a moving window of the most recent elements in the stream
\end{block}
\vspace{-0.2in}
\begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/stream.png}
 \end{center}
\end{frame}

\begin{frame}
\frametitle{Sliding Window --- Two Strategies}
\begin{itemize}
\item \textbf{Re-Execution Strategy}
\begin{itemize}
\item[-] \textbf{Eager Re-execution Strategies} --- Generating new results right after each new data arrives
\item[-] \textbf{Lazy Re-execution Strategies} --- Re-Executing the query periodically
\end{itemize}
\item \textbf{Data Invalidation Strategy}
\begin{itemize}
\item[-] \textbf{Eager Invalidation Strategies} --- Scanning and moving forward the sliding window upon arrival of new data
\item[-] \textbf{Lazy Re-execution Strategies} --- Removing old data periodically and require more memory to store data waiting for expiration
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sliding Window --- Two Strategies}
\begin{itemize}
\item \textbf{Re-Execution Strategy}
\begin{itemize}
\item[-] Eager Re-execution Strategies
\item[-] \textcolor{red}{Lazy Re-execution Strategies}
\end{itemize}
\item \textbf{Data Invalidation Strategy}
\begin{itemize}
\item[-] Eager Expiration Strategies
\item[-] \textcolor{red}{Lazy Invalidation Strategies}
\end{itemize}
\end{itemize}
\vspace{-0.2in}
\textcolor{red}{Re-Execution and Expiration Period --- Generation}
\vspace{-0.2in}
\begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/12.png}
 \end{center}
\end{frame}

\begin{frame}
\frametitle{Different types of dynamic kNN joins}
\begin{itemize}
\item Static R and Dynamic S (SRDS)
\begin{itemize}
\item[-] Exists rarely in real applications. 
\item[-] Reuse the parallel methods
\end{itemize}
\item Dynamic R and Static S (DRSS)
\begin{itemize}

\item[-] Most used scenario in real applications
\item[-] Example: find restaurant for moving users
\item[-] Reuse Random Partition method

\end{itemize}
\item Dynamic R and Dynamic S (DRDS)
\begin{itemize}
\item[-] General situation
\item[-] Example: find Pokémon for moving players
\item[-] Basic Method + Advanced Method
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Dynamic R and Dynamic S --- Basic Method (Sliding Block Nested Loop)}
\textbf{$n^2$ tasks for each generation}
\begin{center}
    	\includegraphics<1>[width=0.6\textwidth]{figs/slidingradompartition.png}
 \end{center}
\end{frame}

\begin{frame}
\frametitle{Dynamic R and Dynamic S --- Advanced Method (Naive Bayes Partitioning)}
\textbf{Purpose: n tasks for each generation: }
\begin{itemize}
\item Partition new data items

\item No time to repartition old ones
\end{itemize}
\textbf{Solution: } Classification for new data items based on Naive Bayes Theory

\end{frame}

\begin{comment}
\begin{frame}
\frametitle{DRDS --- Advanced Method --- Naive Bayes Partitioning}
We consider the n partitions from $N_1$ to $N_n$ as n different classes and the already partitioned data as training set. 
The probability that a new point $x$ belongs to $N_i$ is:
\begin{equation}
P(N_i | x) = \frac{P(x | N_i) \cdot P(N_i)}{P(x)}
\end{equation}

And $x$ should be assigned to the partition $N_y$ which has the biggest probability:
\begin{equation}
x \in N_y, \  where \ P(N_y|x) = max\{P(N_1|x), P(N_2|x), ..., P(N_n|x)\}
\end{equation}
\end{frame}

\begin{frame}
\frametitle{Naive Bayes Partitioning}
Partitioning Problem = The calculation of $P(N_i|x)$
\begin{equation}
P(N_i|x) = P(N_i|(l_1(x), l_2(x), ..., l_p(x)))
\end{equation}
According to Bayes' Theorem:
\begin{equation}
= \frac{P( (l_1(x), l_2(x), ..., l_p(x))|N_i )}{P(  l_1(x), l_2(x), ..., l_p(x) )}
\end{equation}
Since $l_1$, $l_2$, ..., $l_p$ are independent, we have:
\begin{equation}
= \frac{ P(l_1(x)|N_i) \cdot P( l_2(x)|N_i ) ...  \cdot P(l_p(x)|N_i)   \cdot P(N_i)}{P(l_1(x)) \cdot P(l_2(x)) ... \cdot P(l_p(x))   }
\end{equation}

$P(l_j(x)|N_i)$ is the probability of the appearance of $l_j(x)$ on $N_i$, and this probability is decided by the distribution of data on $N_i$.
\end{frame}
\end{comment}




\begin{frame}
\frametitle{Outline}
	\begin{itemize}
		\item Introduction
		\item Parallel Workflow
		\item Theoretical Analysis
		\item Continuous kNN
		\item Experiment Result
		\item Conclusion
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{itemize}

\item A detail survey for parallel kNN join on MapReduce

\item Continuous kNN Join for Data Streams

\item Theoretical and Experimental Analysis

\end{itemize}

\end{frame}


\begin{comment}
\begin{frame}
\frametitle{Conclusion}
\vspace{-0.2in}
\begin{table}
 	\begin{center}\renewcommand{\arraystretch}{1.2}
 	\resizebox{\textwidth}{!}{
 		\begin{tabular}{|c|c|c|c|}
 			\hline
 			\textbf{Algorithm}  & \textbf{Advantage}  &  \textbf{Shortcoming}   & \textbf{Typical Usecase} \\
 			 \hline
 			\textbf{H-BkNNJ} 	& 
 				\begin{tabular}[c]{@{}c@{}}
 					Trivial to implement  
 				\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 					1. Breaks very quickly \\ 
 					2. Optimal parallelism difficult\\ 
 					to achieve a priori
 				\end{tabular} &
 				\begin{tabular}[c]{@{}c@{}}
 			    	Any tiny and low dimension dataset\\
 			    	($\sim$ 25000 records)
 			    \end{tabular} \\ \hline
 			\textbf{H-BNLJ}     &
 				\begin{tabular}[c]{@{}c@{}}
 				 	Easy to implement
 				\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 				 	1. Slow\\
 				 	2. Very large communication overhead 
 				\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 				 	Any small/medium dataset\\
 				 	($\sim$ 100000 records)
 				\end{tabular} \\ \hline
 			\textbf{PGBJ}       & 
	 			\begin{tabular}[c]{@{}c@{}}
 				   	1. Exact solution\\ 
 				   	2. Lowest disk usage\\ 
 			       	3. No impact on communication\\
 			       	overhead with the increase of $k$
	 			\end{tabular}        & 
	 			\begin{tabular}[c]{@{}c@{}}
	 				1. Cannot finish in reasonable time\\ 
	 				for large datasets\\ 
	 				2. Poor performance for high\\
	 				dimension data\\
	 				3. Large communication overhead\\
	 				4. Performance highly depends on\\ 
	 				the quality of a priori chosen pivots
	 			\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 					1. Medium/large dataset for\\
 					low/medium dimension\\
 					2. Exact results
 				\end{tabular} \\ \hline
 			\textbf{H-zkNNJ}    &
	 			\begin{tabular}[c]{@{}c@{}}
	 			    1. Fast\\ 
	 			    2. Does not require a priori parameter\\ 
	 			    tuning \\ 
	 			    3. More precise for large k \\ 
	 			    4. Always give the right number of $k$	 	
	 	        \end{tabular} & 
	 	        \begin{tabular}[c]{@{}c@{}}
	 	        	1. High disk usage\\ 
	 	        	2. Slow for large dimension \\ 
	 	        	3. Very high space requirement ratio \\ 
	 	        	for small values of $k$
 				\end{tabular} & 
 				\begin{tabular}[c]{@{}c@{}}
 					1. Large dataset of small dimension\\
 					2. High values of $k$\\
 			        3. Approximate results
 				\end{tabular} \\ \hline
 			\textbf{RankReduce} & 
 			\begin{tabular}[c]{@{}c@{}}
 				1. Fast\\ 
 				2. Low footprint on disk usage
 			\end{tabular} & 
 			\begin{tabular}[c]{@{}c@{}}
 				1. Fine parameter tuning required with\\ 
 				experimental set up \\
 				2. Multiple hash functions needed for\\
 				acceptable recall \\
 				3. Different quality metrics to consider\\
 				(recall + precision)
 		   \end{tabular}                                           
 			& \begin{tabular}[c]{@{}c@{}}
 				1. Large dataset of any dimension \\
 				2. Approximate results \\
 				3. Room for parameter tuning
 			\end{tabular} \\ \hline
 		\end{tabular}} 
 	\end{center}
 \end{table}
\end{frame}
\end{comment}